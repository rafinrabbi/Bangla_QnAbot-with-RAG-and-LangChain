
# Bangla RAG Chatbot with LangChain & FastAPI

A Retrieval-Augmented Generation (RAG) chatbot for Bangla documents, using Gemini OCR, LangChain, Chroma vector store, and FastAPI.
Source code: [GitHub Public Repo](https://github.com/yourusername/your-repo)

---

## ЁЯЪА Setup Guide

1. **Clone the repo**

   ```sh
   git clone https://github.com/rafinrabbi/Bangla_QnAbot-with-RAG-and-LangChain.git
   cd your-repo
   ```
2. **Create and activate virtual environment**

   ```sh
   python3 -m venv venv
   source venv/bin/activate
   ```
3. **Install dependencies**

   ```sh
   pip install -r requirements.txt
   ```
4. **Set up environment variables**

   - Create a `.env` file with your OpenAI and Gemini API keys:
     ```
     OPENAI_API_KEY=your_openai_key
     GEMINI_API_KEY=your_gemini_key
     ```
5. **Run FastAPI backend**

   ```sh
   uvicorn main:app --reload
   ```
6. **Run Gradio client**

   ```sh
   python3 gradio_app.py
   ```

   The chatbot will be launched and accessible in your browser at the Gradio URL (usually [http://127.0.0.1:7860](http://127.0.0.1:7860)).

   **Sample Chatbot QnA Demo:**

   ![Chatbot QnA Demo](./images/ezgif-504d3516ca9f0f.gif)

---

## ЁЯЫая╕П Used Tools, Libraries, Packages

- **LangChain**: RAG pipeline, chunking, embeddings, conversational memory
- **Chroma**: Vector database for semantic search
- **OpenAI**: Embedding and chat models (`text-embedding-3-large`, `gpt-4o-mini`)
- **FastAPI**: REST API backend
- **Gradio**: Chatbot frontend
- **Gemini OCR**: High-quality Bangla text extraction from images
- **pandas**: Data organization

---

## ЁЯТм Sample Queries & Outputs from the RAG System

**Bangla:**

```
ржкрзНрж░рж╢рзНржи: ржЕржкрж░рж┐ржЪрж┐рждрж╛ ржЧрж▓рзНржкрзЗрж░ рж▓рзЗржЦржХ ржХрзЗ?
ржЙрждрзНрждрж░: ржЕржкрж░рж┐ржЪрж┐рждрж╛ ржЧрж▓рзНржкрзЗрж░ рж▓рзЗржЦржХ рж░ржмрзАржирзНржжрзНрж░ржирж╛рже ржарж╛ржХрзБрж░ред

ржкрзНрж░рж╢рзНржи: ржХрж╛рж░ ржЯрж╛ржХрж╛рж░ ржкрзНрж░рждрж┐ ржЖрж╕ржХрзНрждрж┐ ржмрзЗрж╢рж┐?
ржЙрждрзНрждрж░: ржЕржирзБржкржорзЗрж░ ржорж╛ржорж╛рж░ ржЯрж╛ржХрж╛рж░ ржкрзНрж░рждрж┐ ржЖрж╕ржХрзНрждрж┐ ржмрзЗрж╢рж┐ред рждрж┐ржирж┐ ржзржирзАрж░ ржХржирзНржпрж╛ ржкржЫржирзНржж ржХрж░рзЗржи ржирж╛, ржХрж┐ржирзНрждрзБ ржЯрж╛ржХрж╛рж░ ржкрзНрж░рждрж┐ рждрж╛ржБрж░ ржЖржХрж░рзНрж╖ржг ржЕрж╕рзНржерж┐ржоржЬрзНржЬрж╛ржпрж╝ ржЬржбрж╝рж┐рждред

ржкрзНрж░рж╢рзНржи: 'рждржмрзБ ржЗрж╣рж╛рж░ ржмрж┐рж╢рзЗрж╖ ржорзВрж▓рзНржп ржЖржЫрзЗ' ржПржЦрж╛ржирзЗ ржХрзАрж╕рзЗрж░ ржорзВрж▓рзНржпрзЗрж░ ржХржерж╛ ржмрж▓рж╛ рж╣ржпрж╝рзЗржЫрзЗ?
ржЙрждрзНрждрж░: ржПржЦрж╛ржирзЗ 'рждржмрзБ ржЗрж╣рж╛рж░ ржмрж┐рж╢рзЗрж╖ ржорзВрж▓рзНржп ржЖржЫрзЗ' ржмрж▓рзЗ ржпрзЗ ржорзВрж▓рзНржпрзЗрж░ ржХржерж╛ ржмрж▓рж╛ рж╣ржпрж╝рзЗржЫрзЗ, рждрж╛ ржЬрзАржмржиржХрзЗ ржмрзЛржЭрж╛ржирзЛ рж╣ржпрж╝рзЗржЫрзЗред рж▓рзЗржЦржХ ржЬрзАржмржирзЗрж░ ржжрзИрж░рзНржШрзНржп ржмрж╛ ржЧрзБржгрзЗрж░ ржжрж┐ржХ ржерзЗржХрзЗ ржмржбрж╝ ржирж╛ рж╣рж▓рзЗржУ, рждрж╛рж░ ржоржзрзНржпрзЗ ржПржХржЯрж┐ ржмрж┐рж╢рзЗрж╖ ржорзВрж▓рзНржп ржЖржЫрзЗ, ржпрж╛ ржЬрзАржмржирзЗрж░ ржЕржнрж┐ржЬрзНржЮрждрж╛ ржПржмржВ ржЗрждрж┐рж╣рж╛рж╕рзЗрж░ ржорж╛ржзрзНржпржорзЗ ржкрзНрж░ржХрж╛рж╢ ржкрж╛ржпрж╝ред ржПржЯрж┐ рж╕рзЗржЗ ржЕржирзБржнрзВрждрж┐ ржПржмржВ рж╕рзНржорзГрждрж┐рж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗ, ржпрж╛ ржЬрзАржмржирзЗрж░ ржоржзрзНржпрзЗ ржШржЯрзЗред

```

**English:**

```
Question: What is the main theme of 'Aparichita'?
Answer: 'ржЕржкрж░рж┐ржЪрж┐рждрж╛' ржЧрж▓рзНржкрзЗрж░ ржкрзНрж░ржзрж╛ржи ржерж┐ржо рж╣рж▓ ржпрзМрждрзБржХ ржкрзНрж░ржерж╛рж░ ржмрж┐рж░рзБржжрзНржзрзЗ ржирж╛рж░рзАрж░ ржкрзНрж░рждрж┐рж░рзЛржз ржПржмржВ ржмрзНржпржХрзНрждрж┐рждрзНржмрзЗрж░ ржЬрж╛ржЧрж░ржгред ржЧрж▓рзНржкрзЗ ржХрж▓рзНржпрж╛ржгрзА ржирж╛ржоржХ ржирж╛рж░рзАрж░ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржмрзНржпржХрзНрждрж┐рждрзНржм ржПржмржВ рждрж╛рж░ ржкрж┐рждрж╛рж░ рж╕ржорж░рзНржержирзЗ рж╕ржорж╛ржЬрзЗ ржкрзНрж░ржЪрж▓рж┐ржд ржЕржорж╛ржиржмрж┐ржХ ржпрзМрждрзБржХ ржкрзНрж░ржерж╛рж░ ржмрж┐рж░рзБржжрзНржзрзЗ рж╕ржВржЧрзНрж░рж╛ржорзЗрж░ ржХрж╛рж╣рж┐ржирзА ржмрж░рзНржгрж┐ржд рж╣ржпрж╝рзЗржЫрзЗред ржПржЯрж┐ ржирж╛рж░рзАрж░ рж╕рзНржмрж╛ржзрзАржирждрж╛, ржЖрждрзНржоржорж░рзНржпрж╛ржжрж╛ ржПржмржВ рж╕рж╛ржорж╛ржЬрж┐ржХ ржирзНржпрж╛ржпрж╝ ржкрзНрж░рждрж┐рж╖рзНржарж╛рж░ ржЬржирзНржп ржПржХржЯрж┐ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржмрж╛рж░рзНрждрж╛ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗред.

Question: Who is the real guardian of Anupam?
Answer: ржЕржирзБржкрж╛ржорзЗрж░ ржкрзНрж░ржХрзГржд ржЕржнрж┐ржнрж╛ржмржХ рж╣рж▓ ржорж╛ржорж╛ред

Question: What is the meaning of the phrase "ржлрж▓рзЗрж░ ржорждрзЛ ржЧрзБржЯрж┐"?
Answer: "ржлрж▓рзЗрж░ ржорждрзЛ ржЧрзБржЯрж┐" ржмрж╛ржХрзНржпржЯрж┐ ржмрзЛржЭрж╛рждрзЗ ржЪрж╛ржпрж╝ ржпрзЗ, ржЧрзБржЯрж┐ ржПржХ рж╕ржоржпрж╝ ржкрзВрж░рзНржг ржлрж▓рзЗ ржкрж░рж┐ржгржд рж╣ржпрж╝ред ржХрж┐ржирзНрждрзБ ржЧрзБржЯрж┐ржЗ ржпржжрж┐ ржлрж▓рзЗрж░ ржорждрзЛ рж╣ржпрж╝ рждрж╛рж╣рж▓рзЗ рждрж╛рж░ ржЕрж╕ржорзНржкрзВрж░рзНржг рж╕рж╛рж░ржмрждрзНрждрж╛ ржкрзНрж░ржХржЯ рж╣ржпрж╝рзЗ ржУржарзЗред ржПржЯрж┐ ржирж┐ржЬрзЗрж░ ржирж┐рж╖рзНржлрж▓ ржЬрзАржмржиржХрзЗ ржмрзЛржЭрж╛рждрзЗ ржмрзНржпржмрж╣рзГржд ржПржХржЯрж┐ ржЙржкржорж╛ред

```

---

## ЁЯУС API Documentation

### POST `/ask`

**API URL:** [`http://127.0.0.1:8000/ask`](http://127.0.0.1:8000/ask)

**Example API Request in Postman:**

![Postman API Request Screenshot](./images/postman_api_request.png)

**Request:**

```json
{
  "question": "ржЕржирзБржкржорзЗрж░ ржмрж╛ржмрж╛ ржХрзА ржХрж╛ржЬ ржХрж░рждрзЗржи?",
  "history": ["Who is Anupam?", "What is his family background?"]
}
```

**Response:**

```json
{
  "answer": "ржЕржирзБржкржорзЗрж░ ржмрж╛ржмрж╛ ржУржХрж╛рж▓рждрж┐ ржХрж░рждрзЗржиред"
}
```

---

## ЁЯУК Evaluation Matrix

### Similarity Metric Used

This project uses **cosine similarity** to compare the user queryтАЩs embedding with the stored document chunk embeddings. Cosine similarity measures the angle between two vectors in high-dimensional space, providing a value between -1 and 1 (where 1 means identical direction, i.e., maximum similarity). This metric is widely used for semantic search and retrieval tasks because it is robust to vector magnitude and focuses on the direction (semantic meaning) of the embeddings.

**How it works in this project:**

- The query and each retrieved chunk are embedded using the same embedding model (`text-embedding-3-large`).
- Cosine similarity is computed between the query embedding and each chunk embedding.
- If the maximum similarity score is below a set threshold, the system asks the user for clarification, indicating the query may be too vague or unsupported by the corpus.

**Note:**
Experiments and code for similarity and groundedness evaluation are shown in `rag_evaluation.ipynb`.

---

### Groundedness Measurement

Groundedness in this project refers to whether the chatbotтАЩs answer is accurately supported by the source knowledge or corpus provided to the Retrieval-Augmented Generation (RAG) system. After each user query and generated response, answers are manually checked to determine if, the information in the response directly matches, paraphrases, or closely summarizes content found in the provided knowledge base/textbook. The response avoids hallucinating facts not present in the corpus.

| #  | User Question (Bangla/English)                                                                                                                           | Bot Response (Summary)                                                   | Language | Grounded? |
| -- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | -------- | --------- |
| 1  | What does the phrase "ржПржЗ ржЬрзАржмржиржЯрж╛ ржирж╛ ржжрж╛ржпрж╝рж┐рждрзНржмрзЗрж░ рж╣рж┐рж╕рж╛ржмрзЗ ржмржбрж╝рзЛ, ржирж╛ ржЧрзБржгрзЗрж░ рж╣рж┐рж╕рж╛ржмрзЗ" imply about the character's life? | Explains character feels life is insignificant in both length and value. | English  | тЬЕ Yes    |
| 2  | What does "ржЕржирзНрждржГржкрзБрж░" refer to in Indian mythology?                                                                                               | I don't know.                                                            | English  | тЭМ No     |
| 3  | What is meant by the term "ржЕржирзНрждржГржкрзБрж░" in ancient Indian context?                                                                                  | Explains it's the women's inner quarters in a traditional household.     | English  | тЬЕ Yes    |
| 4  | ржЕржирзБржкржорзЗрж░ ржнрж╛рж╖рж╛ржпрж╝ рж╕рзБржкрзБрж░рзБрж╖ ржХрж╛ржХрзЗ ржмрж▓рж╛ рж╣ржпрж╝рзЗржЫрзЗ?                                                                                 | рж╢рзБржорзНржнрзБржирж╛ржеред                                                     | Bangla   | тЬЕ Yes    |
| 5  | ржХрж╛ржХрзЗ ржЕржирзБржкржорзЗрж░ ржнрж╛ржЧрзНржп ржжрзЗржмрждрж╛ ржмрж▓рзЗ ржЙрж▓рзНрж▓рзЗржЦ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ?                                                                   | ржорж╛ржорж╛ржХрзЗред                                                           | Bangla   | тЬЕ Yes    |
| 6  | ржмрж┐ржпрж╝рзЗрж░ рж╕ржоржпрж╝ ржХрж▓рзНржпрж╛ржгрзАрж░ ржкрзНрж░ржХрзГржд ржмржпрж╝рж╕ ржХржд ржЫрж┐рж▓?                                                                                | рззрзл ржмржЫрж░ред                                                            | Bangla   | тЬЕ Yes    |
| 7  | What does Anupam's father do for a living?                                                                                                               | Anupam's father earns a living by practicing law (ржУржХрж╛рж▓рждрж┐).         | English  | тЬЕ Yes    |
| 8  | What was Promoth ChowdhuryтАЩs contribution to Bengali prose?                                                                                             | I don't know.                                                            | English  | тЭМ No     |
| 9  | kobiguru ke? (Who is "Kobiguru"?)                                                                                                                        | рж░ржмрзАржирзНржжрзНрж░ржирж╛рже ржарж╛ржХрзБрж░ржХрзЗ ржмрзЛржЭрж╛ржирзЛ рж╣ржпрж╝ред              | Mixed    | тЬЕ Yes    |
| 10 | kisher jnno nobel peyechilo? (What did he get the Nobel for?)                                                                                            | For "Gitanjali" and other poetry; first Asian Nobel laureate.            | Mixed    | тЬЕ Yes    |
| 11 | Who is Anupam?                                                                                                                                           | Anupam is a character in "Aparichita," summary given.                    | English  | тЬЕ Yes    |
| 12 | Who is Anupam's father?                                                                                                                                  | Father not named; uncle took responsibility after his death.             | English  | тЬЕ Yes    |
| 13 | What do you know about Anupam?                                                                                                                           | Detailed summary of Anupam's character from "Aparichita."                | English  | тЬЕ Yes    |
| 14 | ржорж╛ржорж╛ржХрзЗ ржнрж╛ржЧрзНржп ржжрзЗржмрждрж╛рж░ ржкрзНрж░ржзрж╛ржи ржПржЬрзЗржирзНржЯ ржмрж▓рж╛ рж╣ржпрж╝рзЗржЫрзЗ ржХрж╛рж░ ржЬржирзНржп?                                                      | ржорж╛ржорж╛рж░ ржкрзНрж░ржнрж╛ржмрзЗрж░ ржЬржирзНржпред                                   | Bangla   | тЬЕ Yes    |
| 15 | ржЕржирзБржкржорзЗрж░ ржЖрж╕рж▓ ржЕржнрж┐ржнрж╛ржмржХ ржХрзЗ?                                                                                                               | ржЕржирзБржкржорзЗрж░ ржЖрж╕рж▓ ржЕржнрж┐ржнрж╛ржмржХ рж╣рж▓рзЛ рждрж╛рж░ ржорж╛ржорж╛ред            | Bangla   | тЬЕ Yes    |
| 16 | What is the age difference between Anupam and his uncle in the story 'Aparichita'?                                                                       | "ржЫржпрж╝ ржмржЫрж░" (six years).                                             | English  | тЬЕ Yes    |
| 17 | ржХрж╛рж░ ржЯрж╛ржХрж╛рж░ ржкрзНрж░рждрж┐ ржЖрж╕ржХрзНрждрж┐ ржмрзЗрж╢рж┐?                                                                                                      | ржЕржирзБржкржорзЗрж░ ржорж╛ржорж╛рж░ ржЯрж╛ржХрж╛рж░ ржкрзНрж░рждрж┐ ржЖрж╕ржХрзНрждрж┐ ржмрзЗрж╢рж┐ред  | Bangla   | тЬЕ Yes    |
| 18 | "Aparichita" ржЧрж▓рзНржкрзЗрж░ ржХрж░рзНржгрзЗрж░ ржкрж╛рж╢рзЗ ржЧржирзНржз ржХрзЗржирзЛ ржЫржбрж╝рж╛ржЫржбрж╝рж┐ рж╣ржпрж╝рзЗржЫрзЗ?                                                         | Explains with literary context and motivations.                          | Bangla   | тЬЕ Yes    |
| 19 | рж░ржмрзАржирзНржжрзНрж░ржирж╛рже ржарж╛ржХрзБрж░ ржХрзЛржи ржЕржнрж┐рж╕ржирзНржзрж╛ржирзЗ рж╕ржорзНржмржирзНржзрж┐ржд рж╣ржпрж╝рзЗржЫрж┐рж▓рзЗржи?                                                     | ржмрж┐рж╢рзНржмржХржмрж┐, Nobel prize, literature related.                       | Bangla   | тЬЕ Yes    |
| 20 | рждрж┐ржирж┐ ржХржд рж╕рж╛рж▓рзЗ ржЬржирзНржоржЧрзНрж░рж╣ржг ржХрж░рзЗржи?                                                                                                      | рззрзорзмрзз рж╕рж╛рж▓рзЗ ржЬржирзНржоржЧрзНрж░рж╣ржг ржХрж░рзЗржиред                          | Bangla   | тЬЕ Yes    |

---

**Legend:**

- тЬЕ Yes: Answer is grounded (supported by corpus).
- тЭМ No: Not grounded / Bot did not answer or hallucinated.

---

## тЭУ Project Questions & Answers

### 1. What method or library did you use to extract the text, and why? Did you face any formatting challenges with the PDF content?

I first tried extracting Bangla text directly from the PDF using **PyMuPDF**, but this resulted in broken fonts and unreadable output (e.g., тАЬржмрзНрж╛рж╝рзЗрж╛ржмрзНрж╛рж░рзНрж╝рзЗ ржХрж┐рж╛рж┐ ржХрж╛рж┐ржХржг ржХржиржпрж╛рж┐ рж░рзНржкрждрж╛тАЭ) due to Bangla encoding issues. I then converted PDF pages to images and used **Tesseract OCR**, which gave partial results but frequently missed important content like tables. Finally, I used **Gemini OCR**, which provided high accuracy for Bangla scripts and reliable extraction even with complex formatting.

**Details of the extraction process and experiments can be found in `data_preparation.ipynb`.**

**Note on Data Cleaning:**
After OCR, most tables (especially MCQ answer tables) were not formatted properly. I had to manually check and fix these tables. Another challenge was that most MCQ answers were presented in a summary table at the end, not with the questions. To automate this, I used GPT with a custom prompt to format the MCQ and table data, matching each answer to its corresponding MCQ. The final format looked like:

рзнрзп. 'ржЦрж╛ржЯрж┐ рж╕рзЛржирж╛ ржмржЯрзЗ!' ржмрж▓рждрзЗ ржмрж┐ржирзБржжрж╛ржжрж╛ ржХрзЛржиржЯрж┐ржХрзЗ ржмрзБржЭрж┐ржпрж╝рзЗржЫрзЗ?
ржХ) ржмржирзЗржжрзА ржШрж░
ржЦ) ржЙржкржпрзБржХрзНржд ржкрж╛рждрзНрж░рзА (ржЙрждрзНрждрж░)
ржЧ) рж╕рзБрж╢рзАрж▓ ржкрж╛рждрзНрж░
ржШ) ржкржгрзЗрж░ ржЧрж╣ржирж╛

Finally, I used the file `./data/extracted_text_gemini/combined_text.txt` as the cleaned and consolidated text for vectorization.

**Details of the extraction process and experiments can be found in `data_preparation.ipynb`.**

### 2. What chunking strategy did you choose? Why do you think it works well for semantic retrieval?

- **Strategy:**I used a **character-based chunking approach** with a chunk size of 1000 characters and an overlap of 200, implemented via LangChainтАЩs `RecursiveCharacterTextSplitter`.
- **Reasoning:**
  This strategy was chosen to maintain coherent context within each chunk and to avoid losing important information at chunk boundaries. The overlap helps preserve sentence continuity, so that relevant content is not split between chunks and lost during retrieval.
  Character-based chunking is also language-agnostic, which works well for Bangla (where sentence boundaries may be harder to detect programmatically), and provides consistent chunk sizes ideal for embedding and semantic similarity search.

### 3. What embedding model did you use? Why did you choose it? How does it capture the meaning of the text?

- **Model:**I initially experimented with `distiluse-base-multilingual-cased-v1` from SentenceTransformers because of its open-source nature and strong multilingual support. However, I found that for more complex queriesтАФespecially when deep semantic context or subtle relationships are involvedтАФthe lower-dimensional embeddings (512) sometimes missed important nuances.
- **Why:**To address this, I switched to OpenAIтАЩs `text-embedding-3-large`, which generates much higher-dimensional embeddings (3072). This provides a richer and more expressive semantic representation, improving similarity search and retrieval accuracy for both Bangla and English.
- **How:**
  The model encodes each text chunk into a high-dimensional vector that reflects its meaning, context, and relationships within the corpus. This allows the system to effectively compare queries and document chunks, even when the queries are complex or phrased differently from the original text.

### 4. How are you comparing the query with your stored chunks? Why did you choose this similarity method and storage setup?

- **Method:**I use cosine similarity to compare the user queryтАЩs embedding with the stored document chunk embeddings. This is implemented via the Chroma vector store, which efficiently indexes and retrieves semantically similar chunks.
- **Why:**
  Chroma is fast, scalable, and integrates seamlessly with LangChain. Cosine similarity is widely adopted for comparing semantic embeddings due to its robustness and interpretability.

### 5. **How do you ensure that the question and the document chunks are compared meaningfully? What would happen if the query is vague or missing context?**

- **Approach:** The query and chunks are embedded using the same model, ensuring semantic alignment. If the query is vague, the retriever may return less relevant chunks, and the LLM will indicate uncertainty or ask for clarification.

### 6. **Do the results seem relevant? If not, what might improve them?**

- **Relevance:** Most of the time, the results seem relevant, particularly for fact-based and context-rich queries. However, in some cases, the model fails to answer correctlyтАФlikely due to issues during text extraction. Before vectorization, further data cleaning and formatting is needed to preserve relationships within the content.
- **Improvements:** For even better results, consider:

  - Finer chunking (e.g., sentence or paragraph level) for more accurate context capture
  - Improved OCR and post-processing to reduce noise in the text

---

## ЁЯУВ Project Structure

```
тФЬтФАтФА main.py                   # FastAPI backend
тФЬтФАтФА gradio_app.py             # Gradio chat client
тФЬтФАтФА data_preparation.ipynb    # Data extraction and cleaning
тФЬтФАтФА rag_evaluation.ipynb      # RAG evaluation and metrics
тФЬтФАтФА requirements.txt
тФЬтФАтФА readme.md
тФЬтФАтФА chroma_db/                # Chroma vector DB files
тФВ   тФЬтФАтФА chroma.sqlite3
тФВ   тФФтФАтФА ...
тФЬтФАтФА data/
тФВ   тФЬтФАтФА HSC26-Bangla1st-Paper.pdf
тФВ   тФЬтФАтФА pdf2img/              # PDF pages as images
тФВ   тФЬтФАтФА extracted_text/       # Tesseract outputs
тФВ   тФФтФАтФА extracted_text_gemini/ # Gemini OCR outputs
тФВ       тФЬтФАтФА combined_text.txt
тФВ       тФЬтФАтФА cleaned_gemini_output.txt
тФВ       тФЬтФАтФА merged_gemini_output.txt
тФВ       тФЬтФАтФА extracted_pages.xlsx
тФВ       тФЬтФАтФА extracted_texts.xlsx
тФВ       тФЬтФАтФА page_1_description.txt
тФВ       тФФтФАтФА ...
тФФтФАтФА __pycache__/              # Python bytecode cache
```

---

<!-- 
## ЁЯСд Author

- [Your Name](https://github.com/yourusername)

---

## ЁЯУД License

MIT -->
