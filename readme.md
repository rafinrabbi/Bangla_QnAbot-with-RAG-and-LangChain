# Bangla RAG Chatbot with LangChain & FastAPI

A Retrieval-Augmented Generation (RAG) chatbot for Bangla documents, using Gemini OCR, LangChain, Chroma vector store, and FastAPI.
Source code: [GitHub Public Repo](https://github.com/yourusername/your-repo)

---

## üöÄ Setup Guide

1. **Clone the repo**

   ```sh
   git clone https://github.com/rafinrabbi/Bangla_QnAbot-with-RAG-and-LangChain.git
   cd your-repo
   ```
2. **Create and activate virtual environment**

   ```sh
   python3 -m venv venv
   source venv/bin/activate
   ```
3. **Install dependencies**

   ```sh
   pip install -r requirements.txt
   ```
4. **Set up environment variables**

   - Create a `.env` file with your OpenAI and Gemini API keys:
     ```
     OPENAI_API_KEY=your_openai_key
     GEMINI_API_KEY=your_gemini_key
     ```
5. **Run FastAPI backend**

   ```sh
   uvicorn main:app --reload
   ```
6. **Run Gradio client**

   ```sh
   python3 gradio_app.py
   ```

   The chatbot will be launched and accessible in your browser at the Gradio URL (usually [http://127.0.0.1:7860](http://127.0.0.1:7860)).

   **Sample Chatbot QnA Demo:**

   ![Chatbot QnA Demo](./images/ezgif-504d3516ca9f0f.gif)

---

## üõ†Ô∏è Used Tools, Libraries, Packages

- **LangChain**: RAG pipeline, chunking, embeddings, conversational memory
- **Chroma**: Vector database for semantic search
- **OpenAI**: Embedding and chat models (`text-embedding-3-large`, `gpt-4o-mini`)
- **FastAPI**: REST API backend
- **Gradio**: Chatbot frontend
- **Gemini OCR**: High-quality Bangla text extraction from images
- **pandas**: Data organization

---

## üí¨ Sample Queries & Outputs from the RAG System

**Bangla:**

```
‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: ‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ ‡¶ó‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶≤‡ßá‡¶ñ‡¶ï ‡¶ï‡ßá?
‡¶â‡¶§‡ßç‡¶§‡¶∞: ‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ ‡¶ó‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶≤‡ßá‡¶ñ‡¶ï ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞‡•§

‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: ‡¶ï‡¶æ‡¶∞ ‡¶ü‡¶æ‡¶ï‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶Ü‡¶∏‡¶ï‡ßç‡¶§‡¶ø ‡¶¨‡ßá‡¶∂‡¶ø?
‡¶â‡¶§‡ßç‡¶§‡¶∞: ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶∞ ‡¶ü‡¶æ‡¶ï‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶Ü‡¶∏‡¶ï‡ßç‡¶§‡¶ø ‡¶¨‡ßá‡¶∂‡¶ø‡•§ ‡¶§‡¶ø‡¶®‡¶ø ‡¶ß‡¶®‡ßÄ‡¶∞ ‡¶ï‡¶®‡ßç‡¶Ø‡¶æ ‡¶™‡¶õ‡¶®‡ßç‡¶¶ ‡¶ï‡¶∞‡ßá‡¶® ‡¶®‡¶æ, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶ü‡¶æ‡¶ï‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶§‡¶æ‡¶Å‡¶∞ ‡¶Ü‡¶ï‡¶∞‡ßç‡¶∑‡¶£ ‡¶Ö‡¶∏‡ßç‡¶•‡¶ø‡¶Æ‡¶ú‡ßç‡¶ú‡¶æ‡¶Ø‡¶º ‡¶ú‡¶°‡¶º‡¶ø‡¶§‡•§

‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: '‡¶§‡¶¨‡ßÅ ‡¶á‡¶π‡¶æ‡¶∞ ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø ‡¶Ü‡¶õ‡ßá' ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶ï‡ßÄ‡¶∏‡ßá‡¶∞ ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡ßá‡¶∞ ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?
‡¶â‡¶§‡ßç‡¶§‡¶∞: ‡¶è‡¶ñ‡¶æ‡¶®‡ßá '‡¶§‡¶¨‡ßÅ ‡¶á‡¶π‡¶æ‡¶∞ ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø ‡¶Ü‡¶õ‡ßá' ‡¶¨‡¶≤‡ßá ‡¶Ø‡ßá ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡ßá‡¶∞ ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶§‡¶æ ‡¶ú‡ßÄ‡¶¨‡¶®‡¶ï‡ßá ‡¶¨‡ßã‡¶ù‡¶æ‡¶®‡ßã ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶≤‡ßá‡¶ñ‡¶ï ‡¶ú‡ßÄ‡¶¨‡¶®‡ßá‡¶∞ ‡¶¶‡ßà‡¶∞‡ßç‡¶ò‡ßç‡¶Ø ‡¶¨‡¶æ ‡¶ó‡ßÅ‡¶£‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶°‡¶º ‡¶®‡¶æ ‡¶π‡¶≤‡ßá‡¶ì, ‡¶§‡¶æ‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø ‡¶Ü‡¶õ‡ßá, ‡¶Ø‡¶æ ‡¶ú‡ßÄ‡¶¨‡¶®‡ßá‡¶∞ ‡¶Ö‡¶≠‡¶ø‡¶ú‡ßç‡¶û‡¶§‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂ ‡¶™‡¶æ‡¶Ø‡¶º‡•§ ‡¶è‡¶ü‡¶ø ‡¶∏‡ßá‡¶á ‡¶Ö‡¶®‡ßÅ‡¶≠‡ßÇ‡¶§‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßç‡¶Æ‡ßÉ‡¶§‡¶ø‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø ‡¶ï‡¶∞‡ßá, ‡¶Ø‡¶æ ‡¶ú‡ßÄ‡¶¨‡¶®‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ò‡¶ü‡ßá‡•§

```

**English:**

```
Question: What is the main theme of 'Aparichita'?
Answer: '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ' ‡¶ó‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶•‡¶ø‡¶Æ ‡¶π‡¶≤ ‡¶Ø‡ßå‡¶§‡ßÅ‡¶ï ‡¶™‡ßç‡¶∞‡¶•‡¶æ‡¶∞ ‡¶¨‡¶ø‡¶∞‡ßÅ‡¶¶‡ßç‡¶ß‡ßá ‡¶®‡¶æ‡¶∞‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∞‡ßã‡¶ß ‡¶è‡¶¨‡¶Ç ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶§‡ßç‡¶¨‡ßá‡¶∞ ‡¶ú‡¶æ‡¶ó‡¶∞‡¶£‡•§ ‡¶ó‡¶≤‡ßç‡¶™‡ßá ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ ‡¶®‡¶æ‡¶Æ‡¶ï ‡¶®‡¶æ‡¶∞‡ßÄ‡¶∞ ‡¶∂‡¶ï‡ßç‡¶§‡¶ø‡¶∂‡¶æ‡¶≤‡ßÄ ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶§‡ßç‡¶¨ ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶∞ ‡¶™‡¶ø‡¶§‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶∞‡ßç‡¶•‡¶®‡ßá ‡¶∏‡¶Æ‡¶æ‡¶ú‡ßá ‡¶™‡ßç‡¶∞‡¶ö‡¶≤‡¶ø‡¶§ ‡¶Ö‡¶Æ‡¶æ‡¶®‡¶¨‡¶ø‡¶ï ‡¶Ø‡ßå‡¶§‡ßÅ‡¶ï ‡¶™‡ßç‡¶∞‡¶•‡¶æ‡¶∞ ‡¶¨‡¶ø‡¶∞‡ßÅ‡¶¶‡ßç‡¶ß‡ßá ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶ï‡¶æ‡¶π‡¶ø‡¶®‡ßÄ ‡¶¨‡¶∞‡ßç‡¶£‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶è‡¶ü‡¶ø ‡¶®‡¶æ‡¶∞‡ßÄ‡¶∞ ‡¶∏‡ßç‡¶¨‡¶æ‡¶ß‡ßÄ‡¶®‡¶§‡¶æ, ‡¶Ü‡¶§‡ßç‡¶Æ‡¶Æ‡¶∞‡ßç‡¶Ø‡¶æ‡¶¶‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶æ‡¶Æ‡¶æ‡¶ú‡¶ø‡¶ï ‡¶®‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∑‡ßç‡¶†‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶¨‡¶æ‡¶∞‡ßç‡¶§‡¶æ ‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶® ‡¶ï‡¶∞‡ßá‡•§.

Question: Who is the real guardian of Anupam?
Answer: ‡¶Ö‡¶®‡ßÅ‡¶™‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶Ö‡¶≠‡¶ø‡¶≠‡¶æ‡¶¨‡¶ï ‡¶π‡¶≤ ‡¶Æ‡¶æ‡¶Æ‡¶æ‡•§

Question: What is the meaning of the phrase "‡¶´‡¶≤‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶ó‡ßÅ‡¶ü‡¶ø"?
Answer: "‡¶´‡¶≤‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶ó‡ßÅ‡¶ü‡¶ø" ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶ü‡¶ø ‡¶¨‡ßã‡¶ù‡¶æ‡¶§‡ßá ‡¶ö‡¶æ‡¶Ø‡¶º ‡¶Ø‡ßá, ‡¶ó‡ßÅ‡¶ü‡¶ø ‡¶è‡¶ï ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶´‡¶≤‡ßá ‡¶™‡¶∞‡¶ø‡¶£‡¶§ ‡¶π‡¶Ø‡¶º‡•§ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶ó‡ßÅ‡¶ü‡¶ø‡¶á ‡¶Ø‡¶¶‡¶ø ‡¶´‡¶≤‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶π‡¶Ø‡¶º ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶§‡¶æ‡¶∞ ‡¶Ö‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∏‡¶æ‡¶∞‡¶¨‡¶§‡ßç‡¶§‡¶æ ‡¶™‡ßç‡¶∞‡¶ï‡¶ü ‡¶π‡¶Ø‡¶º‡ßá ‡¶ì‡¶†‡ßá‡•§ ‡¶è‡¶ü‡¶ø ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶®‡¶ø‡¶∑‡ßç‡¶´‡¶≤ ‡¶ú‡ßÄ‡¶¨‡¶®‡¶ï‡ßá ‡¶¨‡ßã‡¶ù‡¶æ‡¶§‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶â‡¶™‡¶Æ‡¶æ‡•§

```

---

## üìë API Documentation

### POST `/ask`

**API URL:** [`http://127.0.0.1:8000/ask`](http://127.0.0.1:8000/ask)

**Example API Request in Postman:**

![Postman API Request Screenshot](./images/postman_api_request.png)

**Request:**

```json
{
  "question": "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶¨‡¶æ‡¶¨‡¶æ ‡¶ï‡ßÄ ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶§‡ßá‡¶®?",
  "history": ["Who is Anupam?", "What is his family background?"]
}
```

**Response:**

```json
{
  "answer": "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶¨‡¶æ‡¶¨‡¶æ ‡¶ì‡¶ï‡¶æ‡¶≤‡¶§‡¶ø ‡¶ï‡¶∞‡¶§‡ßá‡¶®‡•§"
}
```

---

## üìä Evaluation Matrix

### Similarity Metric Used

This project uses **cosine similarity** to compare the user query‚Äôs embedding with the stored document chunk embeddings. Cosine similarity measures the angle between two vectors in high-dimensional space, providing a value between -1 and 1 (where 1 means identical direction, i.e., maximum similarity). This metric is widely used for semantic search and retrieval tasks because it is robust to vector magnitude and focuses on the direction (semantic meaning) of the embeddings.

**How it works in this project:**

- The query and each retrieved chunk are embedded using the same embedding model (`text-embedding-3-large`).
- Cosine similarity is computed between the query embedding and each chunk embedding.
- If the maximum similarity score is below a set threshold, the system asks the user for clarification, indicating the query may be too vague or unsupported by the corpus.

**Note:**
Experiments and code for similarity and groundedness evaluation are shown in `rag_evaluation.ipynb`.

---

### Groundedness Measurement

Groundedness in this project refers to whether the chatbot‚Äôs answer is accurately supported by the source knowledge or corpus provided to the Retrieval-Augmented Generation (RAG) system. After each user query and generated response, answers are manually checked to determine if, the information in the response directly matches, paraphrases, or closely summarizes content found in the provided knowledge base/textbook. The response avoids hallucinating facts not present in the corpus.

| #  | User Question (Bangla/English)                                                                                                                           | Bot Response (Summary)                                                   | Language | Grounded? |
| -- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | -------- | --------- |
| 1  | What does the phrase "‡¶è‡¶á ‡¶ú‡ßÄ‡¶¨‡¶®‡¶ü‡¶æ ‡¶®‡¶æ ‡¶¶‡¶æ‡¶Ø‡¶º‡¶ø‡¶§‡ßç‡¶¨‡ßá‡¶∞ ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨‡ßá ‡¶¨‡¶°‡¶º‡ßã, ‡¶®‡¶æ ‡¶ó‡ßÅ‡¶£‡ßá‡¶∞ ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨‡ßá" imply about the character's life? | Explains character feels life is insignificant in both length and value. | English  | ‚úÖ Yes    |
| 2  | What does "‡¶Ö‡¶®‡ßç‡¶§‡¶É‡¶™‡ßÅ‡¶∞" refer to in Indian mythology?                                                                                               | I don't know.                                                            | English  | ‚ùå No     |
| 3  | What is meant by the term "‡¶Ö‡¶®‡ßç‡¶§‡¶É‡¶™‡ßÅ‡¶∞" in ancient Indian context?                                                                                  | Explains it's the women's inner quarters in a traditional household.     | English  | ‚úÖ Yes    |
| 4  | ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?                                                                                 | ‡¶∂‡ßÅ‡¶Æ‡ßç‡¶≠‡ßÅ‡¶®‡¶æ‡¶•‡•§                                                     | Bangla   | ‚úÖ Yes    |
| 5  | ‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?                                                                   | ‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶ï‡ßá‡•§                                                           | Bangla   | ‚úÖ Yes    |
| 6  | ‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?                                                                                | ‡ßß‡ß´ ‡¶¨‡¶õ‡¶∞‡•§                                                            | Bangla   | ‚úÖ Yes    |
| 7  | What does Anupam's father do for a living?                                                                                                               | Anupam's father earns a living by practicing law (‡¶ì‡¶ï‡¶æ‡¶≤‡¶§‡¶ø).         | English  | ‚úÖ Yes    |
| 8  | What was Promoth Chowdhury‚Äôs contribution to Bengali prose?                                                                                             | I don't know.                                                            | English  | ‚ùå No     |
| 9  | kobiguru ke? (Who is "Kobiguru"?)                                                                                                                        | ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞‡¶ï‡ßá ‡¶¨‡ßã‡¶ù‡¶æ‡¶®‡ßã ‡¶π‡¶Ø‡¶º‡•§              | Mixed    | ‚úÖ Yes    |
| 10 | kisher jnno nobel peyechilo? (What did he get the Nobel for?)                                                                                            | For "Gitanjali" and other poetry; first Asian Nobel laureate.            | Mixed    | ‚úÖ Yes    |
| 11 | Who is Anupam?                                                                                                                                           | Anupam is a character in "Aparichita," summary given.                    | English  | ‚úÖ Yes    |
| 12 | Who is Anupam's father?                                                                                                                                  | Father not named; uncle took responsibility after his death.             | English  | ‚úÖ Yes    |
| 13 | What do you know about Anupam?                                                                                                                           | Detailed summary of Anupam's character from "Aparichita."                | English  | ‚úÖ Yes    |
| 14 | ‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶ï‡ßá ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶è‡¶ú‡ßá‡¶®‡ßç‡¶ü ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá ‡¶ï‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø?                                                      | ‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø‡•§                                   | Bangla   | ‚úÖ Yes    |
| 15 | ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶Ü‡¶∏‡¶≤ ‡¶Ö‡¶≠‡¶ø‡¶≠‡¶æ‡¶¨‡¶ï ‡¶ï‡ßá?                                                                                                               | ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶Ü‡¶∏‡¶≤ ‡¶Ö‡¶≠‡¶ø‡¶≠‡¶æ‡¶¨‡¶ï ‡¶π‡¶≤‡ßã ‡¶§‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶Æ‡¶æ‡•§            | Bangla   | ‚úÖ Yes    |
| 16 | What is the age difference between Anupam and his uncle in the story 'Aparichita'?                                                                       | "‡¶õ‡¶Ø‡¶º ‡¶¨‡¶õ‡¶∞" (six years).                                             | English  | ‚úÖ Yes    |
| 17 | ‡¶ï‡¶æ‡¶∞ ‡¶ü‡¶æ‡¶ï‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶Ü‡¶∏‡¶ï‡ßç‡¶§‡¶ø ‡¶¨‡ßá‡¶∂‡¶ø?                                                                                                      | ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶∞ ‡¶ü‡¶æ‡¶ï‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶Ü‡¶∏‡¶ï‡ßç‡¶§‡¶ø ‡¶¨‡ßá‡¶∂‡¶ø‡•§  | Bangla   | ‚úÖ Yes    |
| 18 | "Aparichita" ‡¶ó‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶ï‡¶∞‡ßç‡¶£‡ßá‡¶∞ ‡¶™‡¶æ‡¶∂‡ßá ‡¶ó‡¶®‡ßç‡¶ß ‡¶ï‡ßá‡¶®‡ßã ‡¶õ‡¶°‡¶º‡¶æ‡¶õ‡¶°‡¶º‡¶ø ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?                                                         | Explains with literary context and motivations.                          | Bangla   | ‚úÖ Yes    |
| 19 | ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞ ‡¶ï‡ßã‡¶® ‡¶Ö‡¶≠‡¶ø‡¶∏‡¶®‡ßç‡¶ß‡¶æ‡¶®‡ßá ‡¶∏‡¶Æ‡ßç‡¶¨‡¶®‡ßç‡¶ß‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®?                                                     | ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶ï‡¶¨‡¶ø, Nobel prize, literature related.                       | Bangla   | ‚úÖ Yes    |
| 20 | ‡¶§‡¶ø‡¶®‡¶ø ‡¶ï‡¶§ ‡¶∏‡¶æ‡¶≤‡ßá ‡¶ú‡¶®‡ßç‡¶Æ‡¶ó‡ßç‡¶∞‡¶π‡¶£ ‡¶ï‡¶∞‡ßá‡¶®?                                                                                                      | ‡ßß‡ßÆ‡ß¨‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶ú‡¶®‡ßç‡¶Æ‡¶ó‡ßç‡¶∞‡¶π‡¶£ ‡¶ï‡¶∞‡ßá‡¶®‡•§                          | Bangla   | ‚úÖ Yes    |

---

**Legend:**

- ‚úÖ Yes: Answer is grounded (supported by corpus).
- ‚ùå No: Not grounded / Bot did not answer or hallucinated.


---

## üß† Memory Handling: Long-term and Short-term

This project uses both long-term and short-term memory to provide contextually relevant answers:

-**Long-term memory** is handled by the Chroma vector store, which stores all document chunks as embeddings. When a user asks a question, the system retrieves the most semantically similar chunks from this persistent knowledge base, ensuring that information from the entire corpus is available for retrieval at any time.

-**Short-term memory** is managed by LangChain's `ConversationBufferMemory`. This keeps track of the ongoing conversation (chat history) within a session, allowing the model to reference previous user questions and its own answers. This helps the chatbot maintain context, resolve pronouns, and provide more coherent multi-turn interactions.

**How it works together:**

- When a user asks a question, both the current question and the chat history (short-term memory) are passed to the retrieval and generation chain.
- The retriever uses long-term memory to fetch relevant chunks, while the LLM uses both the retrieved context and the chat history to generate a grounded, context-aware answer.

This combination enables the chatbot to answer both isolated and follow-up questions effectively.

---

## ‚ùì Project Questions & Answers

### 1. What method or library did you use to extract the text, and why? Did you face any formatting challenges with the PDF content?

I first tried extracting Bangla text directly from the PDF using **PyMuPDF**, but this resulted in broken fonts and unreadable output (e.g., ‚Äú‡¶¨‡ßç‡¶æ‡¶º‡ßá‡¶æ‡¶¨‡ßç‡¶æ‡¶∞‡ßç‡¶º‡ßá ‡¶ï‡¶ø‡¶æ‡¶ø ‡¶ï‡¶æ‡¶ø‡¶ï‡¶£ ‡¶ï‡¶®‡¶Ø‡¶æ‡¶ø ‡¶∞‡ßç‡¶™‡¶§‡¶æ‚Äù) due to Bangla encoding issues. I then converted PDF pages to images and used **Tesseract OCR**, which gave partial results but frequently missed important content like tables. Finally, I used **Gemini OCR**, which provided high accuracy for Bangla scripts and reliable extraction even with complex formatting.

**Details of the extraction process and experiments can be found in `data_preparation.ipynb`.**

**Note on Data Cleaning:**
After OCR, most tables (especially MCQ answer tables) were not formatted properly. I had to manually check and fix these tables. Another challenge was that most MCQ answers were presented in a summary table at the end, not with the questions. To automate this, I used GPT with a custom prompt to format the MCQ and table data, matching each answer to its corresponding MCQ. The final format looked like:

‡ß≠‡ßØ. '‡¶ñ‡¶æ‡¶ü‡¶ø ‡¶∏‡ßã‡¶®‡¶æ ‡¶¨‡¶ü‡ßá!' ‡¶¨‡¶≤‡¶§‡ßá ‡¶¨‡¶ø‡¶®‡ßÅ‡¶¶‡¶æ‡¶¶‡¶æ ‡¶ï‡ßã‡¶®‡¶ü‡¶ø‡¶ï‡ßá ‡¶¨‡ßÅ‡¶ù‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá?
‡¶ï) ‡¶¨‡¶®‡ßá‡¶¶‡ßÄ ‡¶ò‡¶∞
‡¶ñ) ‡¶â‡¶™‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶™‡¶æ‡¶§‡ßç‡¶∞‡ßÄ (‡¶â‡¶§‡ßç‡¶§‡¶∞)
‡¶ó) ‡¶∏‡ßÅ‡¶∂‡ßÄ‡¶≤ ‡¶™‡¶æ‡¶§‡ßç‡¶∞
‡¶ò) ‡¶™‡¶£‡ßá‡¶∞ ‡¶ó‡¶π‡¶®‡¶æ

Finally, I used the file `./data/extracted_text_gemini/combined_text.txt` as the cleaned and consolidated text for vectorization.

**Details of the extraction process and experiments can be found in `data_preparation.ipynb`.**

### 2. What chunking strategy did you choose? Why do you think it works well for semantic retrieval?

- **Strategy:**I used a **character-based chunking approach** with a chunk size of 1000 characters and an overlap of 200, implemented via LangChain‚Äôs `RecursiveCharacterTextSplitter`.
- **Reasoning:**
  This strategy was chosen to maintain coherent context within each chunk and to avoid losing important information at chunk boundaries. The overlap helps preserve sentence continuity, so that relevant content is not split between chunks and lost during retrieval.
  Character-based chunking is also language-agnostic, which works well for Bangla (where sentence boundaries may be harder to detect programmatically), and provides consistent chunk sizes ideal for embedding and semantic similarity search.

### 3. What embedding model did you use? Why did you choose it? How does it capture the meaning of the text?

- **Model:**I initially experimented with `distiluse-base-multilingual-cased-v1` from SentenceTransformers because of its open-source nature and strong multilingual support. However, I found that for more complex queries‚Äîespecially when deep semantic context or subtle relationships are involved‚Äîthe lower-dimensional embeddings (512) sometimes missed important nuances.
- **Why:**To address this, I switched to OpenAI‚Äôs `text-embedding-3-large`, which generates much higher-dimensional embeddings (3072). This provides a richer and more expressive semantic representation, improving similarity search and retrieval accuracy for both Bangla and English.
- **How:**
  The model encodes each text chunk into a high-dimensional vector that reflects its meaning, context, and relationships within the corpus. This allows the system to effectively compare queries and document chunks, even when the queries are complex or phrased differently from the original text.

### 4. How are you comparing the query with your stored chunks? Why did you choose this similarity method and storage setup?

- **Method:**I use cosine similarity to compare the user query‚Äôs embedding with the stored document chunk embeddings. This is implemented via the Chroma vector store, which efficiently indexes and retrieves semantically similar chunks.
- **Why:**
  Chroma is fast, scalable, and integrates seamlessly with LangChain. Cosine similarity is widely adopted for comparing semantic embeddings due to its robustness and interpretability.

### 5. **How do you ensure that the question and the document chunks are compared meaningfully? What would happen if the query is vague or missing context?**

- **Approach:** The query and chunks are embedded using the same model, ensuring semantic alignment. If the query is vague, the retriever may return less relevant chunks, and the LLM will indicate uncertainty or ask for clarification.

### 6. **Do the results seem relevant? If not, what might improve them?**

- **Relevance:** Most of the time, the results seem relevant, particularly for fact-based and context-rich queries. However, in some cases, the model fails to answer correctly‚Äîlikely due to issues during text extraction. Before vectorization, further data cleaning and formatting is needed to preserve relationships within the content.
- **Improvements:** For even better results, consider:

  - Finer chunking (e.g., sentence or paragraph level) for more accurate context capture
  - Improved OCR and post-processing to reduce noise in the text

---

## üìÇ Project Structure

```
‚îú‚îÄ‚îÄ main.py                   # FastAPI backend
‚îú‚îÄ‚îÄ gradio_app.py             # Gradio chat client
‚îú‚îÄ‚îÄ data_preparation.ipynb    # Data extraction and cleaning
‚îú‚îÄ‚îÄ rag_evaluation.ipynb      # RAG evaluation and metrics
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ readme.md
‚îú‚îÄ‚îÄ chroma_db/                # Chroma vector DB files
‚îÇ   ‚îú‚îÄ‚îÄ chroma.sqlite3
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ HSC26-Bangla1st-Paper.pdf
‚îÇ   ‚îú‚îÄ‚îÄ pdf2img/              # PDF pages as images
‚îÇ   ‚îú‚îÄ‚îÄ extracted_text/       # Tesseract outputs
‚îÇ   ‚îî‚îÄ‚îÄ extracted_text_gemini/ # Gemini OCR outputs
‚îÇ       ‚îú‚îÄ‚îÄ combined_text.txt
‚îÇ       ‚îú‚îÄ‚îÄ cleaned_gemini_output.txt
‚îÇ       ‚îú‚îÄ‚îÄ merged_gemini_output.txt
‚îÇ       ‚îú‚îÄ‚îÄ extracted_pages.xlsx
‚îÇ       ‚îú‚îÄ‚îÄ extracted_texts.xlsx
‚îÇ       ‚îú‚îÄ‚îÄ page_1_description.txt
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ __pycache__/              # Python bytecode cache
```

---

<!-- 
## üë§ Author

- [Your Name](https://github.com/yourusername)

---

## üìÑ License

MIT -->
